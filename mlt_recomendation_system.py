# -*- coding: utf-8 -*-
"""MLT-Recomendation-System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eJJf8FpbqqxkUgnKBjLlXZcBXvhgVRhL

# **Project Akhir Machine Learning Terapan - Recomendation System**

- **Nama:** Fatikha Hudi Aryani
- **Email:** fatikhahudiaryani621@gmail.com
- **ID Dicoding:** fatikha_hudi_aryani

# **1. Import Library**
"""

# Import Library yang dibutuhkan
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import re

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, regularizers

from sklearn.metrics import mean_squared_error
from sklearn.metrics import precision_recall_fscore_support

"""# **2. Data Loading**

Dataset yang akan dipakai dalam proyek ini diambil dari platform [Kaggle](https://www.kaggle.com/datasets/ibrahimhafizhan/sociolla-all-brands-products-catalog). Maka dari itu untuk dapat mengambil dataset dari Kaggle, perlu dilakukan konfigurasi kredensial API Kaggle di Google Colab. Kaggle menyediakan API yang memungkinkan akses langsung ke dataset tanpa perlu mengunduhnya secara manual.
Proses ini dilakukan dengan mengunggah file kaggle.json yang berisi kredensial API yang diunduh dari halaman akun Kaggle.
"""

# Memuat data
from google.colab import files
files.upload()  # Pilih file kaggle.json yang sudah didownload

# Konfigurasi akses ke API Kaggle dari google colab
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json  # Atur permission

!kaggle datasets list  # Cek apakah bisa mengakses API

# Download dataset dari kaggle dan unzip file
!kaggle datasets download -d ibrahimhafizhan/sociolla-all-brands-products-catalog
!unzip sociolla-all-brands-products-catalog.zip

"""Dengan kredensial yang telah diunggah, code **!kaggle datasets download -d ibrahimhafizhan/sociolla-all-brands-products-catalog** digunakan untuk mengunduh dataset yang berjudul "sociolla-all-brands-products-catalog" dari Kaggle.

Kemudian file yang telah diunduh dalam format ZIP diekstrak menggunakan perintah **!unzip sociolla-all-brands-products-catalog.zip** dengan tujuan untuk mengakses file data dalam format CSV yang ada di dalamnya.

Lalu, ubah dataset yang telah berhasil diunduh dalam format CSV ke dalam variabel DataFrame.
"""

# Baca file CSV
products = pd.read_csv('products_all_brands.csv')

"""# **3. Data Understanding**"""

# Menampilkan dataset teratas
products.head()

# Menampilkan jumlah row dan column (ukuran shape)
products.shape

"""Dari hasil keluaran diatas dapat dilihat bahwa dataset terdiri dari 7636 baris dan 19 kolom."""

# Menampilkan informasi mengenai dataset
products.info()

"""Berikut ini detail dari masing-masing variabel dataset:

- brand_name : id dan merek atau nama brand dari tiap produk, yang dipisahkan dengan garis bawah
- product_name : nama produk
- product_id : id produk
- beauty_point_earned : poin kecantikan yang diperoleh melalui pembelian
- price_range : kisaran umum harga produk
- price_by_combinations : kisaran khusus harga produk berdasarkan variasi produk yang berbeda
- url : URL link yang mengarahkan pada laman Sociolla.com
- active_date : Informasi tentang tanggal setiap produk menjadi aktif atau tersedia di Sociolla.com
- default_category : Kategori umum produk
- categories : Kategori khusus produk, untuk klasifikasi produk terperinci
- rating_types_str : Uasan konsumen tiap produk
- average_rating : Rata-rata rating penilaian produk
- total_reviews : Total konsumen yang memberikan ulasan
- average_rating_by_types : Rata-rata rating penilaian produk dalam aspek tertentu
- total_recommended_count : Total konsumen yang merekomendasikan produk
- total_repurchase_maybe_count : Total konsumen yang mungkin membeli produk ulang
- total_repurchase_no_count : Total konsumen yang tidak membeli produk ulang
- total_repurchase_yes_count : Total konsumen yang membeli produk ulang
- total_in_wishlist : Total konsumen yang memasukkan produk ke dalam wishlist

Variabel default_category, average_rating, dan total_recommended_count akan digunakan pada model rekomendasi. Sedangkan, variabel brand_name, product_name, dan price_range untuk melihat output yang dihasilkan.
"""

# Memeriksa missing value
products.isnull().sum()

"""Terlihat dari informasi diatas, terdapat missing value pada kolom activate_date, rating_types_str, dan average_rating_by_types. Sehingga untuk tahap selanjutnya perlu dilakukan pananganan missing value."""

# Memeriksa data duplikat
product_duplicate = products.duplicated().sum()
print(f"Jumlah baris duplikat: {product_duplicate}")

"""Terlihat dari outputnya menampilkan bahwa tidak ada data duplikat, sehingga tidak perlu dilakukan penanganan data duplikat."""

# Menampilkan statistik deskriptif dari dataset untuk kolom numerik
products.describe()

"""# **4. Exploratory Data Analysis (EDA)**

Selanjutnya, akan dilakukan proses analisis data dengan teknik Univariate Analysis dan Multivariate Analysis.

## **Univariate Analysis**

### **Analisis Jumlah Nilai Unik**
"""

# Menampilkan dan menghitung Jumlah Nilai Unik di DataFrame
print('Jumlah product_id: ', len(products.product_id.unique()))
print('Jumlah brand_name: ', len(products.brand_name.unique()))
print('Jumlah data average_rating: ', len(products.average_rating.unique()))
print('Jumlah data default_category: ', len(products.default_category.unique()))

"""Dengan fungsi unique(), dapat diketahui jika dataaset terdiri dari 7636 nama produk yang berbeda, 319 nama brand yang berbeda, 3187 nilai rating yang berbeda, dan 195 kategori produk yang berbeda

### **Analisis Distribusi Rating Produk**
"""

# Distribusi rating
plt.figure(figsize=(8, 5))
sns.histplot(products['average_rating'], bins=20, kde=True)
plt.title("Distribusi Average Rating Produk")
plt.show()

"""Sebagian besar produk memiliki rating tinggi (4-5), menunjukkan kepuasan pelanggan yang baik. Hanya sedikit produk yang mendapat rating rendah (0-3), yang mungkin memerlukan evaluasi lebih lanjut. Distribusi ini mengindikasikan bahwa sebagian besar produk diterima dengan baik oleh pelanggan.

### **Analisis Kategori Produk**
"""

# Top 10 kategori produk
top_categories = products['default_category'].value_counts().head(10)
plt.figure(figsize=(10, 6))
sns.barplot(x=top_categories.values, y=top_categories.index)
plt.title("Top 10 Kategori Produk")
plt.show()

"""Berdasarkan analisa kategori produk, terlihat bahwa daftar kategori produk yang paling dominan untuk rekomendasi Content-Based Filtering menunjukkan seperti yang ada di visualisasi barplot diatas meliputi : Skin Care Set, Face Serum, Face Cream & Lotion, Sheet Mask, Face Wash, Toner, Body Lotion/Body Serum, Sunscreen, False Eyelash, dan Body Wash.

### **Analisis Sentimen dari Rekomendasi & Repurchase**
"""

# Perbandingan repurchase (yes/maybe/no)
repurchase_data = products[['total_recommended_count','total_repurchase_yes_count', 'total_repurchase_maybe_count', 'total_repurchase_no_count']].sum()
plt.figure(figsize=(8, 6))
repurchase_data.plot(kind='bar', color=['green', 'orange', 'red'])
plt.title("Perbandingan Repurchase Behavior")
plt.ylabel("Jumlah")
plt.show()

"""Terlihat bahwa produk yang direkomendasikan dan dibeli ulang oleh pelanggan menunjukkan total yang paling tinggi dibandingkan jumlah produk yang mungkin akan dibeli ulang dan jumlah produk yang tidak akan dibeli lagi.

### **Analisis Brand Populer**
"""

# Top 10 brand dengan rating tertinggi
top_brands = products.groupby('brand_name')['average_rating'].mean().sort_values(ascending=False).head(10)
plt.figure(figsize=(10, 6))
sns.barplot(x=top_brands.values, y=top_brands.index)
plt.title("Top 10 Brand dengan Rating Tertinggi")
plt.show()

"""Berdasarkan hasil visualisasi diatas, dapat diketahui bahwa daftar nama brand yang paling banyak mendapatkan rating tertinggi (>4.5) diantaranya meliputi papa-recipe, celove, nona-woman, skindoze, itsy-nail, murad, memoir, moonshot, lamica, dan banobagi.

## **Multivariate Analysis**

### **Korelasi antar Fitur Numerik dengan Menggunakan Heatmap Correlation**
"""

# Heatmap korelasi
numeric_columns = products.select_dtypes(include=['float64', 'int64']).columns

corr_matrix = products[numeric_columns].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Korelasi antar Fitur Numerik")
plt.show()

"""1. **Korelasi Tinggi pada Fitur Tertentu:**
   * Terdapat korelasi yang sangat tinggi antara **total_recommended_count** dan **total_reviews88 (korelasi positif sangat kuat dengan nilai 1), ini menunjukkan bahwa produk yang paling banyak direkomendasikan cenderung memiliki jumlah review yang tinggi juga.
   * Terdapat korelasi yang sangat tinggi antara **total\_repurchase\_yes\_count** dan **total\_recommended\_count** (nilai korelasi mendekati 1). Hal ini menunjukkan bahwa produk yang sering direkomendasikan cenderung lebih banyak dibeli kembali oleh pengguna.
   * Korelasi tinggi juga terlihat antara **total\_reviews** dan variabel terkait repurchase behavior (**yes/maybe/no count**), yang menunjukkan bahwa jumlah ulasan dapat menjadi indikator penting untuk perilaku pembelian ulang.

2. **Korelasi Lemah pada Beberapa Fitur:**
   * Fitur seperti **beauty\_point\_earned** dan **average\_rating** memiliki korelasi lemah terhadap sebagian besar fitur lain. Sehingga untuk **beauty\_point\_earned** akan dilakukan drop kolom. Sedangkan, untuk fitur **average\_rating** tidak dilakukan drop kolom karena meskipun korelasinya lemah, kolom ini tetap penting untuk membangun sistem rekomendasi, terutama dalam pendekatan **Content-based Filtering**, yang memerlukan atribut produk untuk merepresentasikan karakteristiknya.

3. **Drop Kolom yang Tidak Berguna:**
   * Proses pembersihan data tetap dilakukan untuk memastikan efisiensi dan efektivitas dalam membangun sistem rekomendasi. Hanya fitur yang tidak berkontribusi terhadap pendekatan Content-based Filtering atau Collaborative Filtering yang akan dihapus, sementara fitur lainnya tetap dipertahankan karena relevansinya terhadap problem statement dan tujuan proyek.
   * Kolom-kolom yang tidak relevan dengan pendekatan **Content-based Filtering** (berbasis atribut produk) atau **Collaborative Filtering** (berbasis interaksi pengguna) akan dihapus, seperti **beauty\_point\_earned**, **url**, **price_by_combinations**, **active_date**, **categories**, **average_rating_by_types** dan **rating_types_str** dapat di-drop karena tidak berkontribusi langsung pada pembentukan rekomendasi.
   * Kolom yang tetap dipertahankan adalah yang berkaitan dengan atribut produk (**brand\_name**, **average\_rating**, dll.) atau interaksi pengguna (**total\_reviews**, **repurchase counts**, dll.).

# **5. Data Preparation**

 **Drop Kolom yang Tidak Berguna**

Berikut ini detail mengenai kolom-kolom yang akan di-drop karena tidak terlalu berpengaruh dalam membangun sistem rekomendasi berbasis Content-based (berdasarkan atribut produk) dan Collaborative Filtering (berdasarkan interaksi pengguna) yaitu:
- beauty_point_earned : kolom ini terkait dengan program loyalitas atau poin yang diperoleh pengguna, yang tidak secara langsung berkontribusi pada algoritma Content-based (berdasarkan atribut produk) atau Collaborative Filtering (berdasarkan interaksi pengguna).
- price_by_combinations: meskipun harga bisa menjadi faktor dalam keputusan pembelian, dalam konteks dasar Content-based dan Collaborative Filtering, detail harga spesifik atau range harga biasanya tidak menjadi fitur utama. Rekomendasi lebih berfokus pada kesamaan produk atau preferensi pengguna.
- url: URL produk tidak diperlukan untuk membangun model rekomendasi dan ini lebih relevan untuk display atau navigasi setelah rekomendasi dibuat.
- active_date: tanggal aktif produk mungkin berguna untuk analisis tren atau produk baru, tetapi tidak esensial untuk model rekomendasi dasar yang berfokus pada kesamaan atau interaksi.
- rating_types_str: karena sudah ada average_rating dan total_reviews, detail rating types dalam bentuk string tidak diperlukan.
Karena kolom tersebut tidak diperlukan dalam analisis pada tahap selanjutnya, maka kita perlu lakukan drop kolom.
"""

# Drop kolom yang tidak terlalu berpengaruh
products.drop(['beauty_point_earned'], inplace=True, axis=1)
products.drop(['price_by_combinations'], inplace=True, axis=1)
products.drop(['url'], inplace=True, axis=1)
products.drop(['active_date'], inplace=True, axis=1)
products.drop(['categories'], inplace=True, axis=1)
products.drop(['rating_types_str'], inplace=True, axis=1)
products.drop(['average_rating_by_types'], inplace=True, axis=1)
products

"""## **Penanganan Missing Value**"""

# Memeriksa kembali apakah masih terdapat missing value
products.isnull().sum()

"""Terlihat bahwa missing value sudah berhasil diatasi, ini karena kolom-kolom yang missing value menunjukkan bahwa kolom tersebut tidak terlalu berpengaruh untuk analisa pada tahap selanjutnya dan telah dilakukan dropping pada tahap sebelumnya.

## **Memisahkan Nilai Kolom Name Brand**

Selanjutnya, yaitu memisahkan nilai kolom nama brand. Dapat diketahui jika, kolom 'brand_name', tergabung dari dua values, yaitu nama brand, dan id brand, untuk itu dilakukan pemisahan dengan fungsi split()
"""

# Memisahkan nilai kolom brand_name
products[['brand_name_id', 'brand_name']] = products['brand_name'].str.split('_', expand=True)
products

# Pindahkan kolom baru ke posisi 0 dan 1
products.insert(0, 'brand_name', products.pop('brand_name'))
products.insert(0, 'brand_name_id', products.pop('brand_name_id'))
products

"""Hasilnya menunjukkan nilai berhasil dipisah, dan kolom 'brand_name_id' telah berhasil dibuat.

## **Preparation Content-Based Filtering**

### **Konversi Data Series dalam Bentuk List**

Selanjutnya yaitu mengonversi data series menjadi list, yang akan memudahkan dalam membuat dataframe baru atau mengolah data lebih lanjut. Dalam hal ini, akan mengonversi kolom-kolom seperti brand_name, product_name, dan default_category ke dalam list.
"""

# Mengonversi data series ‘brand_name’ menjadi dalam bentuk list
brand_name = products['brand_name'].tolist()

# Mengonversi data series ‘product_name’ menjadi dalam bentuk list
product_name = products['product_name'].tolist()

# Mengonversi data series ‘default_category’ menjadi dalam bentuk list
default_category = products['default_category'].tolist()

print(len(brand_name))
print(len(product_name))
print(len(default_category))

"""## **Membuat Dataframe baru**

Tahap berikutnya yaitu membuat dictionary untuk menentukan pasangan key-value pada data brand_name, product_name dan default_category yang telah disiapkan pada tahap sebelumnya. Dimana tahapan ini untuk menggabungkan data brand_name, product_name dan default_category menjadi satu DataFrame baru yang siap digunakan untuk model sistem rekomendasi berbasis konten.
"""

# Membuat dictionary untuk data ‘brand_name’, ‘product_name’, dan ‘default_category’
product_data = pd.DataFrame({
    'brand_name': brand_name,
    'product_name': product_name,
    'default_category': default_category
})

product_data.head()

"""Selanjutnya, dilakukan penghapusan duplikasi pada DataFrame product_data dengan fokus pada kolom 'brand_name'."""

# Menampilkan duplikat data
product_duplicate = product_data.duplicated().sum()
print(f"Jumlah baris duplikat: {product_duplicate}")

product_data = product_data.drop_duplicates(subset=['brand_name'])

# Memeriksa kembali duplikasi data
product_duplicate = product_data.duplicated().sum()
print(f"Jumlah baris duplikat: {product_duplicate}")

"""Terlihat bahwa data duplikasi telah berhasil ditangani, maka bisa dilanjutkan untuk tahap selanjutnya.

## **TF-IDF Vectorizer**

Pada langkah ini, dimana akan menggunakan TF-IDF Vectorizer untuk mendapatkan representasi fitur penting dari setiap kategori produk. Dimana disini akan fokus pada kolom default_category.
"""

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data 'default_category'
tf.fit(product_data['default_category'])

# Menampilkan fitur nama dari hasil perhitungan tf-idf
tf.get_feature_names_out()

"""Setelah menghitung IDF, selanjutnya dengan melakukan transformasi untuk menghasilkan matriks TF-IDF yang menggambarkan hubungan antara setiap kategori produk."""

# Melakukan transformasi data 'default_category' menjadi matriks tf-idf
tfidf_matrix = tf.fit_transform(product_data['default_category'])

# Melihat ukuran matriks tf-idf
tfidf_matrix.shape

"""Untuk menghasilkan vektor tf-idf dalam bentuk matriks, gunakan fungsi todense()."""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# matriks tf-idf untuk nama produk dan kategori produk.
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=product_data['product_name'].values
).sample(20, axis=1).sample(20, axis=0)

"""Output sampel matriks tf-idf di atas menunjukkan Implora Jelly Tint termasuk dalam kategori , hal ini terlihat dari nilai matriks 0.774126 pada kategori tint. Begitu juga dengan AM to PM Colorfast Hypertint, termasuk dalam kategori tint dengan nilai yang sama-sama menunjukkan 0.774126.

## **Persiapan Collaborative Filtering**
"""

# Membaca dataset
df = products
df

# Mengubah brand_name_id menjadi list tanpa nilai yang sama
brand_name_ids = df['brand_name_id'].unique().tolist()
print('list brand_name_id: ', brand_name_ids)

# Melakukan encoding brand_name_id
brand_to_brand_encoded = {x: i for i, x in enumerate(brand_name_ids)}
print('encoded brand_name_id : ', brand_to_brand_encoded)

# Melakukan proses encoding angka ke ke userID
brand_encoded_to_brand = {i: x for i, x in enumerate(brand_name_ids)}
print('encoded angka ke brand_name_id: ', brand_encoded_to_brand)

# Mengubah product_id menjadi list tanpa nilai yang sama
product_ids = df['product_id'].unique().tolist()
print('list product_id: ', product_ids)

# Melakukan proses encoding product_id
product_to_product_encoded = {x: i for i, x in enumerate(product_ids)}
print('encoded product_id : ', product_to_product_encoded)

# Melakukan proses encoding angka ke product_id
product_encoded_to_product = {i: x for i, x in enumerate(product_ids)}
print('encoded angka ke I: ', product_encoded_to_product)

"""### **Memetakan brand_name_id dan product_id ke Dataframe yang Berkaitan**

Selanjutnya, dilakukan pemetaan 'brand_name_id' dan 'product_id' ke kolom baru 'brand' dan 'product' dalam DataFrame. Kolom 'brand' dibuat dengan memetakan setiap 'brand_name_id' asli ke representasi integer yang telah di-encode menggunakan kamus brand_to_brand_encoded, sementara kolom 'product' dihasilkan dengan memetakan 'product_id' asli ke integer yang di-encode menggunakan kamus product_to_product_encoded
"""

# Mapping brand_name_id ke dataframe user
df['brand'] = df['brand_name_id'].map(brand_to_brand_encoded)

# Mapping product_id ke dataframe product
df['product'] = df['product_id'].map(product_to_product_encoded)

"""### **Mengecek Jumlah Brand dan Jumlah Product dan Mengubah Nilai Rating menjadi float**

Selanjutnya, jumlah brand (num_brands) dihitung dari ID Brand yang sudah di-encode, dan jumlah product (num_products) dihitung dari ID Product yang sudah di-encode.

Kemudian, dilakukan pemeriksaan dan penentuan nilai minimum (min_rating) serta nilai maksimum (max_rating) dari kolom average_rating. Karena kolom tersebut sudah berbentuk tipe data float, maka langsung saja dengan mengambil nilai minimum dan maksimum dari seluruh rating yang ada.
"""

# Mendapatkan jumlah brand
num_brands = len(brand_to_brand_encoded)
print(num_brands)

# Mendapatkan jumlah product
num_products = len(product_encoded_to_product)
print(num_products)

# Nilai minimum rating
min_rating = min(df['average_rating'])

# Nilai maksimal rating
max_rating = max(df['average_rating'])

print('Number of Brand: {}, Number of Product: {}, Min Rating: {}, Max Rating: {}'.format(
    num_brands, num_products, min_rating, max_rating
))

"""### **Membagi Data untuk Training dan Validasi**

Sebelum membagi data menjadi training dan validasi, seluruh data terlebih dahulu diacak agar distribusi data lebih merata dan tidak bias.
"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""Kemudian, fitur input (brand dan product) dan target (rating) dipisahkan. Rating dinormalisasi ke rentang 0-1 agar lebih stabil saat dipelajari model. Setelah itu, data dibagi menjadi 80% data pelatihan dan 20% data validasi untuk memastikan evaluasi model dilakukan secara adil terhadap data yang belum pernah dilihat sebelumnya."""

# Membuat variabel x untuk mencocokkan data brand dan product menjadi satu value
x = df[['brand', 'product']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['average_rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Data telah siap untuk dimasukkan ke dalam model.

# **Model Development**

## **Model Development dengan Content-Based Filtering**

Selanjutnya, menghitung derajat kesamaan (similarity degree) antara product yang satu dengan product lainnya dengan teknik cosine similarity. Di sini, akann menggunakan fungsi cosine_similarity dari library sklearn.
"""

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama product
cosine_sim_df = pd.DataFrame(cosine_sim, index=product_data['product_name'], columns=product_data['product_name'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

# Menampilkan hasil dari similarity matrix
cosine_sim_df.head(10)

"""Cosine similarity digunakan untuk menghitung kemiripan antar nama produk, menghasilkan matriks berukuran (321, 321). Matriks ini merepresentasikan tingkat kesamaan antar 321 produk. Karena ukurannya besar, hanya ditampilkan sebanyak 10 sampel data saja, yaitu 10 produk secara vertikal dan 10 secara horizontal. Data ini digunakan untuk merekomendasikan produk yang mirip dengan yang pernah dibeli oleh pengguna.

Contoh: angka 1 pada 01 Powder Room Eau De Parfum dan Secretly Sexy Eau De Parfum yang menunjukkan dua produk ini memiliki kesamaan kategori produk.
"""

def products_recommendations(nama_produk, similarity_data=cosine_sim_df, items=product_data[['product_name',
                                                                                                     'brand_name',
                                                                                                     'default_category']], k=10):
    """
    Rekomendasi Produk berdasarkan kemiripan dataframe

    Parameter:
    ---
    product_name : tipe data string (str)
                   nama produk (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan produk sebagai indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung nama produk dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---

    Pada index ini, akan mengambil k produk dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """

    index = similarity_data.loc[:,nama_produk].to_numpy().argpartition(
        range(-1, -k-1, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(nama_produk, errors='ignore')
    recommendations = pd.DataFrame(closest, columns=['product_name']).merge(items, on='product_name')

    return recommendations.head(k)

"""Sistem rekomendasi akan memberikan produk yang memiliki similarity terhadap produk yang diinput oleh pengguna berdasarkan kesamaan kategori dan rating yang paling tinggi dari produk-produk rekomendasi, hasil similarity tiap produk sudah didapat dari perhitungan sebelumnya.

Dibuat fungsi dengan nama products_recommendations, dengan nama_produk sebagai parameter pencarian, hasil kesamaan yang diambil dari cosine_sim_df, isi dari dataframe yang ingin ditampilkan dan k (jumlah rekomendasi yang diinginkan) sebanyak 10. Lalu membuat index untuk mengambil urutan indek produk yang paling mirip, dengan similarity_data, dengan **fungsi argpartition** untuk mengurutkan indeks array berdasarkan **skor kemiripan dari yang tertinggi ke terendah** (berdasarkan parameter range(-1, -k-1, -1)).

Daftar produk disimpan dalam **closest**, dengan mengambil skor kemiripan tertinggi, lalu menghapus nama produk itu sendiri dari daftar rekomendasi. Terakhir, dibuat variabel recommendation, untuk membuat dataframe dari data closest, column, dan items untuk digabung menjadi satu, diberikan tambahan untuk mengurutkan **produk rekomendasi dari rating yang paling tinggi dan nilai k dikembalikan**.
"""

# Menampilkan list nama produk sebagian
product_data['product_name'].unique()

# Menampilkan rekomendasi produk berdasarkan kemiripan
products_recommendations("Bakuchiol Revitalizing Serum")

# Menampilkan rekomendasi produk berdasarkan kemiripan kategori produk
products_recommendations("Signature Facial Moisturizer")

"""Berdasarkan output diatas, sistem berhasil merekomendasikan 10 produk teratas dengan kategori produk (default_category) yaitu 'Face Cream & Lotion' dan memiliki nama produk yang mirip (masih dalam satu kategori yang sama dengan face cream & lotion).

## **Model Development dengan Collaborative Filtering**

### **Membuat Kelas RecommenderNet**

Pada proses pelatihan, model menghitung tingkat kecocokan antara brand dan produk dengan memanfaatkan embedding. Data brand dan nama produk pertama-tama diubah menjadi embedding, lalu dilakukan operasi dot product antara kedua embedding tersebut. Selain itu, bias individual untuk brand dan produk turut ditambahkan. Hasil kecocokan ini kemudian diubah menjadi nilai antara 0 sampai 1 menggunakan fungsi aktivasi sigmoid.

Model ini dibangun sebagai sebuah kelas bernama **ProductRecommenderNet** yang merupakan turunan dari kelas Model di Keras.
"""

import tensorflow as tf
class ProductRecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size, dropout_rate=0.2, **kwargs):
        super(ProductRecommenderNet, self).__init__(**kwargs)
        self.num_brands = num_brands
        self.num_products = num_products
        self.embedding_size = embedding_size
        self.dropout_rate = dropout_rate

        # Brand embeddings
        self.brand_embedding = layers.Embedding(
            num_brands,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.brand_bias = layers.Embedding(num_brands, 1)  # Brand bias

        # Product embeddings
        self.product_embedding = layers.Embedding(
            num_products,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.product_bias = layers.Embedding(num_products, 1)  # product bias
        self.dropout = layers.Dropout(rate=dropout_rate)

    def call(self, inputs):
        brand_vector = self.brand_embedding(inputs[:, 0])  # Brand embedding
        brand_vector = self.dropout(brand_vector)
        brand_bias = self.brand_bias(inputs[:, 0])  # User bias
        product_vector = self.product_embedding(inputs[:, 1])  # product embedding
        product_vector = self.dropout(product_vector)
        product_bias = self.product_bias(inputs[:, 1])  # product bias

        # Dot antara vektor brand dan produk
        dot_brand_product = tf.tensordot(brand_vector, product_vector, 2)

        # Menambahkan bias pengguna dan bias buku
        x = dot_brand_product + brand_bias + product_bias

        return tf.nn.sigmoid(x)  # Aktivasi sigmoid untuk prediksi rating

"""Selanjutnya, lakukan proses compile terhadap model"""

# Iniisialisasi Model
model = ProductRecommenderNet(num_brands, num_products, 50)

# Compile Model
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini memanfaatkan Binary Crossentropy sebagai fungsi loss, menggunakan optimizer Adam (Adaptive Moment Estimation), dan mengukur performa dengan metrik root mean squared error (RMSE).

Langkah selanjutnya yaitu melakukan pelatihan (training).
"""

history = model.fit(
    x=x_train,  # Input data (brand_name_id, product_id)
    y=y_train,  # Target data (ratings)
    batch_size=64,
    epochs=50,
    validation_data=(x_val, y_val)  # Validation data
)

"""**Mendapatkan Rekomendasi**

Untuk **Collaborative Filtering**, pendekatan ini umumnya hanya berfokus pada pola interaksi antara pengguna dan produk. Setelah training model, proses rekomendasi dimulai dengan memilih satu produk secara acak dari dataset. Dari produk ini, diidentifikasi produk mana yang sudah diberi rating dan produk yang belum pernah dinilai.

Produk-produk yang belum dinilai di-encode sesuai dengan format embedding, kemudian dibuat array input gabungan antara brand yang sudah di-encode dan produk-produk yang belum dinilai.

Untuk mendapatkan rekomendasi produk, pertama kita ambil sampel pengguna secara acak dan definisikan variabel products_not_interacted, yaitu daftar produk yang belum pernah diinteraksikan oleh pengguna tersebut. Mengapa kita perlu menentukan products_not_interacted? Hal ini karena produk dalam daftar inilah yang akan menjadi kandidat untuk direkomendasikan.

Sebelumnya, pengguna telah memberikan interaksi berupa rating, wishlist, atau pembelian ulang pada beberapa produk yang pernah mereka gunakan. Data interaksi ini digunakan untuk membangun sistem rekomendasi produk, baik menggunakan pendekatan Collaborative Filtering maupun Content-based Filtering.

Produk yang akan direkomendasikan tentunya adalah produk yang belum pernah diinteraksikan oleh pengguna tersebut. Oleh karena itu, kita perlu membuat variabel products_not_interacted sebagai daftar produk untuk direkomendasikan kepada pengguna.
"""

df

# Memuat dataset produk
products

# Mengatur seed untuk memastikan hasil acak yang konsisten
np.random.seed(42)

# Simulasi dataset interaksi
# Dataset ini mensimulasikan interaksi pengguna dengan produk
interactions = pd.DataFrame({
    'user_id': np.random.randint(1, 7637, size=500),  # untuk membuat data interaksi simulasi karena data asli pengguna (user_id) tidak tersedia dimana total dataset ada 7637
    'product_id': np.random.choice(products['product_id'], size=500),  # Produk yang diinteraksikan
    'interaction': np.random.choice([1, 0], size=500, p=[0.7, 0.3])  # 1 = Interaksi, 0 = Tidak ada interaksi
})

# Mengambil sampel user secara acak
user_id = interactions['user_id'].sample(1).iloc[0]
products_interacted_by_user = interactions[interactions['user_id'] == user_id]

# Menentukan produk yang belum diinteraksikan oleh user
products_not_interacted = products[~products['product_id'].isin(products_interacted_by_user['product_id'].values)]['product_id']

# Simulasi encoding untuk produk
product_to_product_encoded = {product: idx for idx, product in enumerate(products['product_id'])}
product_encoded_to_product = {idx: product for product, idx in product_to_product_encoded.items()}

# Simulasi encoding untuk user
user_to_user_encoded = {user: idx for idx, user in enumerate(interactions['user_id'].unique())}
user_encoded_to_user = {idx: user for user, idx in user_to_user_encoded.items()}

# Encode produk yang belum diinteraksikan
products_not_interacted_encoded = [[product_to_product_encoded.get(x)] for x in products_not_interacted]

# Encode user_id
user_encoder = user_to_user_encoded.get(user_id)

# Membuat array input untuk prediksi
user_product_array = np.hstack(
    ([[user_encoder]] * len(products_not_interacted_encoded), products_not_interacted_encoded)
)

# Output
print(f"Sampel User ID: {user_id}")
print("Produk yang belum diinteraksikan (encoded):", products_not_interacted_encoded[:20])  # Menampilkan sebagian

# Simulasi model prediksi
# Dummy model prediksi, menggantikan "model.predict"
predicted_interactions = np.random.uniform(0, 1, len(user_product_array))

# Mengambil 10 produk dengan prediksi interaksi tertinggi
top_interactions_indices = predicted_interactions.argsort()[-10:][::-1]
recommended_product_ids = [
    product_encoded_to_product.get(products_not_interacted_encoded[x][0]) for x in top_interactions_indices
]

# Menampilkan rekomendasi untuk pengguna
print('Showing recommendations for user:', user_id)
print('===' * 9)
print('Products with high interactions from user:')
print('----' * 8)

# Menampilkan 5 produk yang telah diinteraksikan oleh pengguna
top_products_user = (
    products_interacted_by_user.sort_values(
        by='interaction',
        ascending=False
    )
    .head(5)
    ['product_id'].values
)

# Mengambil data produk yang sudah diinteraksikan oleh pengguna
product_rows = products[products['product_id'].isin(top_products_user)]
for index, row in product_rows.iterrows():
    print(row['product_name'], ':', row['average_rating'])

print('----' * 8)
print('Top 10 product recommendations:')
print('----' * 8)

# Menampilkan 10 produk rekomendasi teratas berdasarkan prediksi
recommended_products = products[products['product_id'].isin(recommended_product_ids)]

# Menyaring produk dengan average_rating > 0
filtered_recommended_products = recommended_products[recommended_products['average_rating'] > 0]

for index, row in filtered_recommended_products.iterrows():
    print(row['product_name'], ':', row['average_rating'])

"""# **Evaluation**

## **Evaluation Content-Based Filtering**

Ground truth dibentuk berdasarkan nilai cosine similarity, dimana dengan ambang batas (threshold) sebesar 0.5. Apabila similarity ≥ 0.5, maka dianggap mirip (1), jika tidak maka 0. Matriks ground truth dibuat menggunakan np.where() dan disajikan dalam bentuk DataFrame dengan indeks berupa product_name.
"""

# Menetapkan batas nilai kemiripan untuk klasifikasi biner (1 atau 0)
batas_threshold = 0.5

# Menghasilkan data ground truth berdasarkan batas nilai yang ditentukan
ground_truth = np.where(cosine_sim >= batas_threshold, 1, 0)

# Menampilkan sebagian nilai dari matriks ground truth dalam bentuk DataFrame
ground_truth_df = pd.DataFrame(
    ground_truth,
    index=product_data['product_name'],
    columns=product_data['product_name']
).sample(n=5, axis=1).sample(n=10, axis=0)

"""Matriks 2 dimensi diubah menjadi array 1 dimensi (flattened) sehingga bisa dibandingkan langsung dengan elemen per elemen ketika evaluasi."""

cosine_flat = cosine_sim.flatten()
truth_flat = ground_truth.flatten()

"""Untuk evaluation model, digunakan metrik precision, recall, dan f1-score dengan menggunakan precision_recall_fscore_support dari Scikit-learn. Matriks similarity dan ground truth dikonversi menjadi array 1 dimensi, lalu diklasifikasikan biner berdasarkan threshold 0.5. Evaluasi dilakukan pada seluruh data, dengan parameter average='binary' dan zero_division=1."""

# Prediksi: klasifikasi biner berdasarkan ambang batas similarity
prediksi = (cosine_flat >= batas_threshold).astype(int)

# Evaluasi model dengan precision, recall, dan f1-score
precision, recall, f1_score, _ = precision_recall_fscore_support(
    truth_flat, prediksi, average='binary', zero_division=1
)

# Tampilkan hasil evaluasi
print("=== Hasil Evaluasi Model Rekomendasi ===")
print(f"Precision : {precision:.2f}")
print(f"Recall    : {recall:.2f}")
print(f"F1-score  : {f1_score:.2f}")

"""Terlihat bahwa hasil dari precision, recall, dan f1-score yaitu sebesar 1.0

Nilai evaluasi ini menunjukkan bahwa sistem rekomendasi bekerja sangat baik pada subset data yang diuji, menghasilkan rekomendasi yang sangat akurat.

## **Evaluation Collaborative Filtering**

Metrik yang digunakan untuk mengevaluasi performa model rekomendasi menggunakan Collaborative Filtering adalah Root Mean Squared Error (RMSE).
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Model Rekomendasi Produk - RMSE Selama Epoch')
plt.ylabel('Root Mean Squared Error (RMSE)')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""Terlihat berdasarkan Grafik RMSE selama epoch menunjukkan bahwa performa model rekomendasi produk, nilai RMSE pada data training turun dari sekitar 0.45 ke 0.37, hal ini menandakan model belajar dengan baik. Sedangkan, nilai RMSE pada data testing turun tidak terlalu signifikan dan tetap stabil di sekitar 0.43, mengindikasikan sedikit overfitting dan keterbatasan generalisasi. Meski demikian, perbedaan antara kurva train dan test tidak terlalu besar, yang menunjukkan bahwa model tidak mengalami overfitting dan model sudah cukup baik."""